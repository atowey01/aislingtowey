---
title: "Car Manufacturer Dataset Analysis"
author: "Aisling Towey"
date: "22 March 2019"
output: rmarkdown::github_document
---
###########
##Project Summary
The objective of this project is to carry out analysis on a car manufacturer's data to predict the time it takes to complete a check on each car before it is released from the manufacturer. The purpose of this prediction is to provide recommendations on how the time for each check can be reduced.

##Background
The data provided consists of 4209 observations and 378 variables. The outcome variable labelled 'y' contains the time it takes to complete a check of each car and the data also contains and ID variable for each check. The remaining variables are all binary variables excluding 8 rows containing factor variables. The names of these remaining variables are labeled from X0 to X385 giving no indication of the meaning of the variables.


```{r warning = TRUE, echo = FALSE, results='hide', include=FALSE}

##Load packages 
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(corrplot)) install.packages("corrplot")
if(!require(funModeling)) install.packages("funModeling")
if(!require(dplyr)) install.packages("dplyr")
if(!require(Hmisc)) install.packages("Hmisc")
if(!require(caret)) install.packages("caret")
if(!require(dummies)) install.packages("dummies")
if(!require(car)) install.packages("car")
if(!require(e1071)) install.packages("e1071")
if(!require(ranger)) install.packages("ranger")
if(!require(logisticPCA)) install.packages("logisticPCA")
if(!require(factoextra)) install.packages("factoextra")
if(!require(factoMineR)) install.packages("FactoMineR")
if(!require(xgboost)) install.packages("xgboost")
if(!require(skimr)) install.packages("skimr")
if(!require(gridExtra)) install.packages("gridExtra")
if(!require(grid)) install.packages("grid")
library(gridExtra)
library(grid)
library(funModeling) 
library(Hmisc)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(corrplot)
library(Matrix)
library(dplyr)
library(caret)
library(dummies)
library(e1071)
library(ranger)
library(logisticPCA)
library(factoextra)
library(FactoMineR)
library(xgboost)
```


```{r echo = FALSE, results='hide'}
## read in the data
car <- read.csv("train.csv")
```

##Process of Analysis
The initial step taken in the analysis was to identify the type of analysis that was required. As the outcome variable is continuous and the aim is to predict the variable, it is evident that this is a regression prediction problem.

The next step was to carry out exploratory data analysis which involved checking for missing values, checking for multicollinearity, identifying outliers, removing zero variance columns, creating plots of variables, encoding factor variables and exploring the response variable.

Following the exploratory data analysis phase, different models were evaluated to identify the most appropriate for this scenario. The models selected were:

* Linear Regression
* XGBoost
* Random Forest

Different variations of the selected models were trialied by including principal component analysis and altering the tuning and parameters of the models. As dimensionality is a key feature of this dataset with many variables, principal component analysis was also identified as a useful technique to reduce dimensionality. Before running the models the data was split into a training and test set. 80% of the data was put into the training set and 20% into the test set. The training set was used to train the model and the test set was used to ensure the model generalized well on unseen data.  R-squared is the metric used in this analysis to select the best model. Higher r-squared values signify better models. R-squared is deemed more appropriate in this scenario than root mean squared error (RMSE) as it is a relative metric with values ranging from 0 to 1 which makes it easier to interpret. RMSE is used to support the r-squared findings. Lower RMSE values signify better models.


```{r echo = FALSE, eval=FALSE, results='hide', cache=TRUE}

###################### Exploratory data anlysis section ####################################

##complete basic exploratory data analysis 
str(car)
head(car)
dim(car)

glimpse(car)
summary(car)
skim(car)
describe(car)

```


```{r echo = FALSE, eval=FALSE, results='hide'}
##explore split of factor variables by plotting
freq(car)
```


```{r echo = FALSE, eval=FALSE, results='hide'}
#explore response variable (minimum and maximum)
min(car$y)
max(car$y)
```


```{r echo = FALSE, eval=FALSE, results='hide'}
#create histogram of response variable
ggplot(car, aes(x = y))+
  ggtitle("Histogram of Response Variable")+
  theme(plot.title = element_text(hjust = 0.5))+
  geom_histogram(fill = "pink3")
```


```{r echo = FALSE, eval=FALSE, results='hide'}
#create boxplot of response variable
boxplot(car$y)$out
```


```{r echo = FALSE, eval=FALSE, results='hide'}
##check correlations between numeric variables and then plot
#do this for all variables in separate graphs so they are readable
#identify columns where correlations are not running due to the column having zero variance (all 0's)
corrvar<-round(cor(car[,11:50]) ,2)
corrplot(corrvar, method = "circle")

```


```{r echo = FALSE, results='hide'}
#remove outlier identified above and zero variance columns identified
#we ensure zero variance columns are removed when running the models by specifying "zv" in preprocessing
car <- car %>%
  filter(y<250) %>% 
  select(-c(ID, X11, X93,X107,X233,X235,X268,X289,X290,X293,X297,X330,X347))
```

```{r echo = FALSE, eval=FALSE, results='hide'}
#replot response variable to ensure outlier has been removed
ggplot(car, aes(x = y))+
  geom_histogram()
```


```{r echo = FALSE, results='hide'}
#create dummy variables of all 9 factor columns in the dataset so they can be used in the models
dumVar <- dummyVars(" ~ .", data = car)
car <- data.frame(predict(dumVar, newdata = car))

#check the dimensions of the data and the head of the data to ensure this has worked
dim(car)
head(car)
```


```{r echo = FALSE, eval=FALSE, results='hide'}
#now calculate correlation matrix on all predictors in the dataset to check for multicolinearity
corMatrix <- cor(car[,1:552])
#summarize the correlation matrix
print(corMatrix)
#find attributes that are highly corrected
highCor <- findCorrelation(corMatrix, cutoff=0.8)
# print indexes of highly correlated attributes
print(highCor)

#there is multicolinearity which will be dealt with when running the models
```



```{r echo = FALSE, results='hide'}

################################### Model preparation section ####################################

#create training and testing data, use 80:20 split
in_train <- createDataPartition(car$y, p = 0.8, list = FALSE)
car_train <- car[in_train, ]
car_test <- car[-in_train, ]

#check the structure of the train and test set so ensure the split has occurred
dim(car)
dim(car_train)
dim(car_test)
```



```{r echo = FALSE, results='hide'}
# separate predictor variables from the response variable (`y`)
car_x <- car_train %>% 
  select(-y) 

car_y <- car_train %>% 
  select(y) %>% 
  pull()

# double check whether the correct variables are selected
head(car_x)
head(car_y)
```


```{r echo = FALSE, results='hide'}
#create train/test indexes which will be used in 5-Fold CV
myFolds_regression <- createFolds(car_y, k = 5)

#inspect myFold indexes
myFolds_regression
```


```{r echo = FALSE, results='hide'}
#create unique configuration which will be shared across all regression models 
ctrl_regression <- trainControl(
  method = "cv", # used for configuring resampling method: in this case cross validation 
  number = 5, # instruct that it is 5 fold-cv
  index = myFolds_regression, # folds' indexes
  verboseIter = TRUE, # print output of each step
  savePredictions = TRUE, 
  preProcOptions = list(thresh = 0.8) # in case that PCA preprocessing option is selected in the train() function
  # indicates a cutoff for the cumulative percent of variance to be retained by PCA
)
```



```{r echo = FALSE, eval=FALSE, results='hide'}

######################################## PCA section ####################################

#to remove zero variance columns from the dataset for pca, use the apply expression, setting variance not equal to zero
#this will ensure pca will run
pca_train <- car_train[,apply(car_train, 2, var)!=0]

#perform PCA only on the data and print summary
car_pca <- prcomp(pca_train, center = TRUE, scale. = TRUE)
summary(car_pca)
```


```{r echo = FALSE, eval=FALSE, results='hide'}

#extract the matrix of variable loadings (i.e. eigenvectors)
car_pca$rotation

# extract: 
  # 1. eigenvalues, 
  # 2. proportion of variances (in %) explained by each PC
eig <- get_eig(car_pca)
eig

#notes 139 principal components should be retained to keep 80% of variance

#create scree plot of proportion of variances (in %) explained by each PC
fviz_eig(car_pca)
res_var <- get_pca_var(car_pca) 

#calculate correlation between each variable and each PCi 
res_var$cor

#the proportion of variability in an original variable explained by each principal component PCi
res_var$cos2

#the proportion of variability in an original variable explained by retaining 139 principal components
prop_var <- res_var$cos2[,1:139]

#combine the variability explained to prop_var to get clearer data
prop_var <- cbind(prop_var, variability_explained = rowSums(prop_var))

#create plot
fviz_pca_var(car_pca, col.var = "cos2", gradient.cols = "lancet")

#show the head of the dataset
head(car_pca)

#show the principal component scores for each of these observations
head(car_pca$x)

#earlier we discovered that we should retain the first 139 PCs
#if we wish to account for over 80% of the overall variability and if we would like to train ML model using PCA transformed dataset, we should select relevant variables
#please note pca can be added to the models using the caret package without this step
car_train_pca_transformed <- car_pca$x[, 1:139] %>% 
  as.data.frame() %>% 
  mutate(
    y = car_train$y # we need to merge response variable from the train dataset with the PCs
  )

# using the previously trained PCA model, calculate the principal component scores of observations in the test dataset
car_test_pca_transformed <- predict(car_pca, car_test[,1:552]) 
car_test_pca_transformed

#earlier we discovered that we should retain the first 139 PCs if we wish to account for over 80% of the overall variability
#similarly to the train dataset, we should select the relevant variables
car_test_pca_transformed <- car_test_pca_transformed[, 1:139] %>% 
  as.data.frame() %>% 
  mutate(
    y = car_test$y # we need to merge response variable from the train dataset with the PCs
  )

car_test_pca_transformed

```


```{r echo = FALSE, results='hide', cache=TRUE, include=FALSE}

################################### Linear regression section ####################################

#set a seed for reproducibility
set.seed(123)

#perform data-preprocessing step which will perform data centering & scaling and remove variables with zero variance
#train LM model using default CARET parameters
#check r-squared values for best model and RMSE to support
model_lm_car_default <- train(
  x = car_x, # predictors dataset
  y = car_y, # response variable
  method = "lm", 
  trControl = ctrl_regression,
  tuneLength = 10,# training configuration
  preProcess = c("zv", "center", "scale") # zv - remove predictors with zero variance
  # center, scale - centering and scaling data 
)

#model summary
model_lm_car_default

#inspect the variable importance, in the default lm model
plot(varImp(model_lm_car_default))

#get LM coefficients of the final model
summary(model_lm_car_default$finalModel)

#predict on test data
test_lm_default <- predict(model_lm_car_default, newdata = select(car_test, -y))

#get r-squared and RMSE of the model
R2(pred = test_lm_default, obs = select(car_test,y))
RMSE(pred = test_lm_default, obs = car_test$y)

```


```{r echo = FALSE, include=FALSE, results='hide', cache=TRUE}

#we will also use PCA dimensionality reduction technique retaining 139 principal components to account for over 80% of the overall variability in the dataset

#set a seed for reproducibility
set.seed(123)

#train the model
model_car_pca <- train(
  x = car_x, #predictors dataset
  y = car_y, #response variable
  method = "lm", #ML algorithm
  trControl = ctrl_regression,
  tuneLength = 10,
  preProcess = c("zv", "center", "scale", "pca") #zv - remove predictors with zero variance
  #center, scale - centering and scaling data 
  #pca - perform PCA transformation on input dataset (retain only those PCs that explain 80% variance)
)

#model summary
model_car_pca

#inspect the variable importance, in the default LM model
plot(varImp(model_car_pca))

#get LM coefficients of the final model
summary(model_car_pca$finalModel)

#predict on the test set
test_lm_pca <- predict(model_car_pca, newdata = select(car_test, -y))

#get r-squared and RMSE of the model
R2(pred = test_lm_pca, obs = select(car_test,y))
RMSE(pred = test_lm_pca, obs = car_test$y)

```


```{r echo = FALSE, include=FALSE, results='hide', cache=TRUE}

################################### XGBoost section ####################################

#set a seed for reproducibility
set.seed(123)

#set the parameters for the grid to be used in tunegrid
tune_grid <- expand.grid(nrounds = 200, 
                         max_depth = 5, 
                         eta = 0.05, 
                         gamma = 0.01, 
                         colsample_bytree = 0.75, 
                         min_child_weight = 0, 
                         subsample = 0.5)

#train the XGBoost model using default CARET parameters
model_xgb <- train(y ~., data = car_train, method = "xgbTree", 
                trControl = ctrl_regression, 
                tuneGrid = tune_grid, 
                tuneLength = 10)

#check model output
model_xgb

#predict using the test set
set.seed(123)
test_xgb <- predict(model_xgb, newdata = select(car_test, -y))

#get r-squared and RMSE of the model
R2(pred = test_xgb, obs = select(car_test,y))
RMSE(pred = test_xgb, obs = car_test$y)
```



```{r echo = FALSE, include=FALSE, results='hide', cache=TRUE}

################################### Random Forest section ####################################

#set a seed for reproducibility
set.seed(123)

#train the random forest model using default CARET parameters
model_ranger_default <- train(
  x = car_x, # predictors dataset
  y = car_y, # response variable
  method = "ranger", 
  trControl = ctrl_regression, # training configuration
  importance = "impurity", # this needs to be added only for `ranger` for identifying variable importance
  preProcess = c("zv", "center", "scale") # zv - remove predictors with zero variance
  # center, scale - centering and scaling data 
)

#model summary
model_ranger_default

#check the optimal hyperparameter value(s)
model_ranger_default$bestTune

#check the models RMSE results and select the model with lowest RMSE
model_ranger_default$results[which.min(model_ranger_default$results$RMSE), ]
model_ranger_default$results
```


```{r echo = FALSE, include=FALSE, results='hide', cache=TRUE}

#instead of randomly selecting mtree values 
#let's instruct CARET to randomly select 20 different mtree values, and select the one for which the model has the highest AUC score
model_ranger_auto <- train(
  x = car_x, # predictors dataset
  y = car_y, # response variable
  method = "ranger",
  trControl = ctrl_regression, # training configuration
  importance = "impurity", # this needs to be added only for `ranger` for identifying variable importance
  tuneLength = 20, # caret's random selection of tuning parametres
  preProcess = c("zv", "center", "scale") # zv - remove predictors with zero variance
  # center, scale - centering and scaling data 
)

#model summary
model_ranger_auto

#the optimal hyperparameter value(s)
model_ranger_auto$bestTune

#we can see that the model `model_ranger_auto` performs better than the model `model_ranger_default`
model_ranger_default$results[which.max(model_ranger_default$results$ROC), ]
model_ranger_auto$results[which.max(model_ranger_auto$results$ROC), ]
model_ranger_auto$results[which.min(model_ranger_auto$results$RMSE), ]
model_ranger_auto$results

#inspect the impact of different hyperparameter settings on predictive perforormances of these two models
plot(model_ranger_default)
plot(model_ranger_auto)

#inspect the variable importance, in the default model
plot(varImp(model_ranger_default))
plot(varImp(model_ranger_auto))

```


```{r echo = FALSE, results='hide', cache=TRUE, include=FALSE}

#based on the results of the auto model, manually specify mtry and min.node.size to try and improve the model

#train the random forest model using default CARET parameters
model_ranger_manual <- train(
  x = car_x, # predictors dataset
  y = car_y, # response variable
  method = "ranger", 
  trControl = ctrl_regression, # training configuration
  importance = "impurity", # this needs to be added only for `ranger` for identifying variable importance
  tuneGrid = expand.grid(
    mtry = 24:69,
    splitrule = c("gini", "extratrees"),
    min.node.size = 5
  ),
  preProcess = c("zv", "center", "scale") # zv - remove predictors with zero variance
  # center, scale - centering and scaling data 
)

#model summary
model_ranger_manual

#the optimal hyperparameter value(s)
model_ranger_manual$bestTune

#inspect the impact of different hyperparameter settings on predictive perforormances of the model
plot(model_ranger_manual)

model_ranger_manual$results[which.min(model_ranger_manual$results$RMSE), ]
model_ranger_manual$results[which.max(model_ranger_manual$results$Rsquared), ]
model_ranger_manual$results

```



```{r echo = FALSE, results='hide'}
#using the manaul random forest model (identified as the best model of the random forest models), make predictions using the test set
ranger_predictions<-predict(model_ranger_manual,newdata=select(car_test, -y))# return predicted classes
car_test$y <- as.numeric(car_test$y)

#get r-squared and RMSE of the model
R2(pred = ranger_predictions, obs =select(car_test, y))
RMSE(pred = ranger_predictions, obs = car_test$y) 
```

```{r echo = FALSE, include = FALSE}
# plot the observed dataset against the predicted data
# black line = observed
# red line = predicted
data.frame(
  id = 1:length(car_test$y),
  observed = car_test$y,
  predicted = ranger_predictions
) %>% 
  ggplot()+
  geom_line(aes(x =id, y = observed))+
  labs(title = "Random Forest Model - Observed Values (Black) v Predicted Values (Red)", y = "time")+
  geom_line(aes(x = id, y = predicted), colour = "red")

```


```{r echo = FALSE, results='hide', include = FALSE}

############################# Compare all models section ######################################

set.seed(123)

#compare performance of the models
lm_car_resample <- resamples(
  list(
    lm_default = model_lm_car_default,
    lm_pca = model_car_pca,
    xgb = model_xgb, 
    ranger_default = model_ranger_default,
    ranger_auto = model_ranger_auto,
    ranger_manual = model_ranger_manual
  )
)

#comparison summary
summary(lm_car_resample)

#create plots to compare the comparisons
bwplot(lm_car_resample, metric = 'Rsquared')
```

##Results
The results of the models as illustrated in figure 1 show the random forest model achieves the highest r-squared value of 0.56 and the lowest RMSE value of 8.28. When the model is used to predict the outcome of the test set, the random forest model achieves an r-squared value of 0.57 and an RMSE value of 8.28 meaning the model generalized well on new data.

```{r echo = FALSE, out.width="90%", out.height="30%", fig.align="center", fig.cap="Comparison of models showing random forest as the best model"}
#create plots to compare the comparisons
rsqdot <- dotplot(lm_car_resample, metric = 'Rsquared', main = "Model Comparison - R^2")
rmsedot <- dotplot(lm_car_resample, metric = 'RMSE', main = "Model Comparison - RMSE")

grid.arrange(rsqdot, rmsedot, ncol = 2)
```

The variables in the dataset found to be most important to the random forest model are X127, X261 and X314 as in figure 2. The model relies significantly more on these variables than any other variables in the dataset. These variables all occur more than 1700 times in the dataset meaning they appear in at least 40% of checks.

```{r echo = FALSE, out.width="55%", out.height="55%", fig.align="center", fig.cap="Relative importance of top 6 variables the random forest model relies on"}
#plot the 10 most important variables of the manual random forest model
vimp_manual<- varImp(model_ranger_manual)
plot(vimp_manual, 6, ylab = "Variable", main = "Variable Importance")
```

```{r echo = FALSE, eval=FALSE, results='hide'}
#check how often the most important variables are 1
sum(car$X127)
sum(car$X314)
sum(car$X261)
dim(car)
```


##Recommendations 
The results of this analysis suggest that focusing of the variables X127, X261 and X314 when conducting checks may be beneficial in reducing the time taken to complete each check. An understanding of the meaning of these variables would be required to make more specific recommendations of what this may involve.

##Limitations
The main limitation of this analysis is that there is no codebook or data dictionary available explaining the meaning of each variable. An understanding of the variables would allow for more specific models, increased understanding of the results and improved recommendations. 


